{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"0730_NeuralNetwork.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"tiPv16RfdiPf","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wEdhatj_iWPe","colab_type":"text"},"source":["## tanh(x) <=> sigmoid  서로 표현 가능하다\n","\n","## tanh(x) = 2* sigmoid(x) -1"]},{"cell_type":"code","metadata":{"id":"QsvcBFDNinWO","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D0cd1j-_jUdW","colab_type":"text"},"source":["## tanh activation\n","\n","## ReLU\n","### f(x) = max(0,x)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tlj7_GN9qDE2","colab_type":"text"},"source":["## 가고싶은 방향: 'zig zag path'\n","\n","## dead ReLU\n","\n","## Leaky ReLU\n","### f(x) = max(0.01x,x)  특정어플리케이션에 사용\n","\n","## ELU\n","### f(x) = \n","\n","###"]},{"cell_type":"markdown","metadata":{"id":"Lme7NXhRucE5","colab_type":"text"},"source":["# Linear하다의 정의:\n","- f(m+n) = f(m)+f(n)"]},{"cell_type":"code","metadata":{"id":"OjNORfPSuh3-","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ChEbcVcDxG7W","colab_type":"text"},"source":["# MLP\n","\n","# Cost Function / Object Function\n","- 어떤 문제를 줬을 때 기계가 정답을 맞추는데 \n","목적이 위에 두 function을 낮추는일!\n","- 목적으로 가기위해 사용하는게 gradient descent 알고리즘 -> 최저점을 찾기 위함\n","- backpropergation알고리즘\n","\t=> 이걸 사용하기 위해서는 각 레이어마다 수치가 있어야돼\n","\n","# Activation Function\n","(ex) sigmoid, relu\n","\n","# Regularization\n","모방하게 됨으로써 생길 수 있는 오버피팅문제\n","- 모방하지 말게 하기 위해서 “Regularization\"\n","\n","# 왜 리니어보다 non리니어를 사용해야하는지?\n","- 리니어를 여러개 쌓아도 결국 치환되서 하나의 리니어 형태로 변환되는 문제점 존재\n","- \n","\n","* gradient vanishing\n","- 그래프 중간에 flat한 부분이 0이 되는데 이런 부분에서는 gradient가 없어지게 되니까 이걸 뜻함\n","\n","# ReLU\n","- 0보다 큰 부분에 대해서는 gradient가 1이 나와\n","- 가장 큰 특징 : saturate 되지 않는다 -> gradient0이 되지 않는다\n","\t-> 항상 gradient가 1의 값을 가지기 때문\n","- converage가 더 빠르다\n","\t-> 원하는 목표로 도달하게 해주는데 더 빠르다\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uA0vI9ZexODI","colab_type":"text"},"source":["## Gradient descent vs Stochastic\n","## "]},{"cell_type":"code","metadata":{"id":"zohwN6qHxHYV","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9RpP6tVSEjBs","colab_type":"text"},"source":["# Gradient Descent Update\n","- 등고선을 직선으로 잘라준다. \n","- \n","\n","# Momentum Update\n","- x -= learning_rate * dx\n","- v = mu * v - learning_rate * dx\n","- x += v\n",">\n","- m2 = muv1 = lim xdx1\n","- m3 = \n","\n","# Adagrad Update\n","- cache += dx**2\n","- x -= learning_rate * dx / (np.sqrt(cache) + 1e-7)\n","\n","#RMSProp Update\n","- cache += decay_rate * cache + (1-decay_rate) * dx**2\n","- x -= learning_rate * dx / (np.sqrt(cache) + 1e-7)\n","\n","first_moment = 0\n","\n","second_moment = 0\n","\n","while True:\n","    dx = compute_gradient(x)\n","    first_moment = beta1 * first_moment + (1 - beta1) * dx\n","    \n","   first_moment = beta1 * first_moment + (1 - beta1) * dx"]},{"cell_type":"markdown","metadata":{"id":"0pge5x0yTgw4","colab_type":"text"},"source":[""]}]}